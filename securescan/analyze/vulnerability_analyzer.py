"""Vulnerability analyzer using Claude Opus 4.6.

Takes raw findings from static analysis and uses LLM reasoning to:
- Confirm or reject each finding
- Trace data flow from source to sink
- Assess severity and exploitability
- Provide detailed reasoning for each determination
"""

from __future__ import annotations

import logging

from securescan.analyze.codebase_digest import CodebaseDigest
from securescan.analyze.opus_client import LLMResponse, OpusClient
from securescan.detect.models import (
    EnrichedFinding,
    Exploitability,
    RawFinding,
    Severity,
)

logger = logging.getLogger(__name__)

ANALYSIS_SYSTEM_PROMPT = """\
You are a senior application security engineer performing a code audit.
You have deep expertise in identifying and explaining security vulnerabilities
in Python and JavaScript applications.

Your analysis must be:
- Precise: only flag genuine vulnerabilities, not theoretical risks
- Evidence-based: cite specific code lines and data flow paths
- Actionable: explain what's wrong and how to fix it

When analyzing a finding, trace the complete data flow from input source
to dangerous sink. Consider whether sanitization, validation, or framework
protections exist that would mitigate the issue.

Always respond with valid JSON matching the requested schema. No preamble
or explanation outside the JSON object."""


def _build_analysis_prompt(
    finding: RawFinding,
    digest: CodebaseDigest,
    repo_name: str,
) -> str:
    """Build the user prompt for vulnerability analysis."""

    finding_context = ""
    for section in digest.sections:
        if finding.file_path in section.heading:
            finding_context = section.content
            break

    if not finding_context:
        finding_context = finding.code_snippet

    return f"""\
Analyze this potential security vulnerability found in the repository "{repo_name}".

## Detection Details
- **Vulnerability Type:** {finding.vuln_type.value}
- **File:** {finding.file_path}
- **Lines:** {finding.line_start}-{finding.line_end}
- **Detection Method:** {finding.detection_method.value}
- **Rule:** {finding.rule_id or 'custom'}
- **Initial Assessment:** {finding.message}

## Code Context
{finding_context}

## Codebase Overview
{digest.sections[0].content if digest.sections else 'Not available'}

## Task
Determine whether this is a genuine vulnerability. Analyze:
1. Is there a real source of untrusted input flowing to this dangerous operation?
2. Does any sanitization, validation, or framework protection exist?
3. Is this code reachable from external input (HTTP request, CLI arg, file input)?
4. What is the realistic impact if exploited?

Respond ONLY with a JSON object in this exact schema:
{{
    "is_vulnerable": true or false,
    "confidence": 0.0 to 1.0,
    "severity": "critical" or "high" or "medium" or "low",
    "cvss_estimate": 0.0 to 10.0,
    "reasoning": "detailed explanation of your analysis",
    "taint_chain": "source -> transformation -> ... -> sink (or null if not vulnerable)",
    "exploitability": "confirmed" or "likely" or "possible" or "unlikely",
    "blast_radius": "description of what could be compromised",
    "is_reachable": true or false or null
}}"""


_SEVERITY_MAP = {
    "critical": Severity.CRITICAL,
    "high": Severity.HIGH,
    "medium": Severity.MEDIUM,
    "low": Severity.LOW,
}

_EXPLOITABILITY_MAP = {
    "confirmed": Exploitability.CONFIRMED,
    "likely": Exploitability.LIKELY,
    "possible": Exploitability.POSSIBLE,
    "unlikely": Exploitability.UNLIKELY,
}


def _parse_analysis_response(
    finding: RawFinding,
    response: LLMResponse,
) -> EnrichedFinding | None:
    """Parse the LLM analysis response into an EnrichedFinding."""

    if not response.success or not response.json_data:
        logger.warning(
            f"LLM analysis failed for {finding.file_path}:{finding.line_start}: "
            f"{response.error or 'No JSON in response'}"
        )
        return EnrichedFinding(
            raw=finding,
            severity=Severity.MEDIUM,
            cvss_estimate=5.0,
            reasoning="LLM analysis unavailable - manual review recommended",
            taint_chain=None,
            exploitability=Exploitability.POSSIBLE,
            blast_radius="Unknown - analysis failed",
            is_reachable=None,
        )

    data = response.json_data

    if not data.get("is_vulnerable", True):
        logger.info(
            f"LLM rejected finding {finding.file_path}:{finding.line_start} - "
            f"not vulnerable ({data.get('reasoning', 'no reason given')[:100]})"
        )
        return None

    return EnrichedFinding(
        raw=finding,
        severity=_SEVERITY_MAP.get(data.get("severity", "medium"), Severity.MEDIUM),
        cvss_estimate=float(data.get("cvss_estimate", 5.0)),
        reasoning=data.get("reasoning", "No reasoning provided"),
        taint_chain=data.get("taint_chain"),
        exploitability=_EXPLOITABILITY_MAP.get(
            data.get("exploitability", "possible"), Exploitability.POSSIBLE
        ),
        blast_radius=data.get("blast_radius", "Unknown"),
        is_reachable=data.get("is_reachable"),
    )


def analyze_findings(
    client: OpusClient,
    findings: list[RawFinding],
    digest: CodebaseDigest,
    repo_name: str,
) -> list[EnrichedFinding]:
    """Analyze raw findings using Opus 4.6 semantic reasoning."""

    enriched: list[EnrichedFinding] = []
    rejected_count = 0

    logger.info(f"Analyzing {len(findings)} findings with Opus 4.6...")

    for index, finding in enumerate(findings, 1):
        logger.info(
            f"  [{index}/{len(findings)}] {finding.vuln_type.value} in "
            f"{finding.file_path}:{finding.line_start}"
        )

        prompt = _build_analysis_prompt(finding, digest, repo_name)

        response = client.analyze(
            system_prompt=ANALYSIS_SYSTEM_PROMPT,
            user_prompt=prompt,
            max_tokens=2048,
            temperature=0.0,
            expect_json=True,
        )

        result = _parse_analysis_response(finding, response)

        if result is not None:
            enriched.append(result)
            confidence = response.json_data.get("confidence", "?") if response.json_data else "?"
            logger.info(
                f"    -> {result.severity.value.upper()} "
                f"(CVSS {result.cvss_estimate:.1f}, confidence {confidence})"
            )
        else:
            rejected_count += 1
            logger.info("    -> Rejected (not a real vulnerability)")

    logger.info(
        f"Analysis complete: {len(enriched)} confirmed, "
        f"{rejected_count} rejected out of {len(findings)} raw findings"
    )

    return enriched
